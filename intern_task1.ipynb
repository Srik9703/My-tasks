{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Srik9703/My-tasks/blob/main/intern_task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SYy-fyVMCor0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEKPO3cbxObH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0H-QuuyuC92w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jxabs4j5HVnM"
      },
      "outputs": [],
      "source": [
        "#extracting urls from sitemap\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def parse_sitemap(sitemap_path):\n",
        "  try:\n",
        "    #using get req to get data\n",
        "    f=requests.get(sitemap_path)\n",
        "    k=f.content\n",
        "    #using xml parser to find all links\n",
        "    soup = BeautifulSoup(k, 'xml')\n",
        "    urls_list = []\n",
        "    #getting all the links\n",
        "    loc_tags =soup.find_all('loc')\n",
        "    #adding all links into a list\n",
        "    for i in loc_tags:\n",
        "          urls_list.append(i.get_text())\n",
        "    #print(\"no.of urls=\",len(urls_list))\n",
        "\n",
        "    return list(set(urls_list))\n",
        "  except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20hsn0bngkyn"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o3rhvvQxnWdv"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_html_and_extract_text(html_text):\n",
        "  try:\n",
        "    #using a html parser\n",
        "      soup=BeautifulSoup(html_text,'html.parser',from_encoding=\"iso-8859-1\")\n",
        "\n",
        "      for data in soup(['style', 'script']):\n",
        "          # Remove tags\n",
        "          data.decompose()\n",
        "\n",
        "      #retriving only inner text of an element\n",
        "      k=' '.join(soup.stripped_strings)\n",
        "      #removing all special characters\n",
        "      without_spcl_char=re.sub(r'[^a-zA-Z0-9\\s]+', '', k)\n",
        "      #removing extra whitespaces\n",
        "      cleaned_text=\" \".join(without_spcl_char.split())\n",
        "\n",
        "      return cleaned_text.lower()\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YYs2aKn6ucy7"
      },
      "outputs": [],
      "source": [
        "#to extract content from url\n",
        "import requests\n",
        "\n",
        "def fetch_html_content(url):\n",
        "  try:\n",
        "    data=requests.get(url)\n",
        "    return data.content\n",
        "  except Exception as e:\n",
        "        print('There was an error in fetching {}: {}'.format(url, e))\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kng2o594vGdP"
      },
      "outputs": [],
      "source": [
        "#to save extracted html and text content into file\n",
        "import os\n",
        "from os.path import exists\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def save_to_file(text,path,name):\n",
        "  try:\n",
        "    path+=\"/html_text\"\n",
        "    if not os.path.exists(folder_path):\n",
        "          os.makedirs(folder_path)\n",
        "    #check if file already exsists\n",
        "    if exists(name):\n",
        "          print('HTML file already exists')\n",
        "          return\n",
        "\n",
        "\n",
        "      #fun=open(os.path.join(path,name),\"w\")\n",
        "    with open(os.path.join(path,name),\"w\") as file:\n",
        "            file.write(str(text))\n",
        "  except Exception as e:\n",
        "        print(f'An error occurred while saving to file {name}: {e}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse, unquote\n",
        "\n",
        "def generate_file_name(url):\n",
        "    # Parse URL to get path\n",
        "    path = urlparse(url).path\n",
        "    # Remove special characters and decode URL encoding\n",
        "    filename = unquote(\"\".join([c if c.isalnum() or c in ('_', '-') else '_' for c in path]))\n",
        "    # Limit filename length to avoid issues\n",
        "    filename = filename[:255]  # Adjust the limit as needed\n",
        "    return filename"
      ],
      "metadata": {
        "id": "3sKBjgboLPjB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "def download_pdf(url,folder_path):\n",
        "        folder_path+=\"/pdfs\"\n",
        "        if not os.path.exists(folder_path):\n",
        "              os.makedirs(folder_path)\n",
        "        filename=generate_file_name(url)+\".pdf\"\n",
        "        if exists(filename):\n",
        "          print('file already exists')\n",
        "          return\n",
        "\n",
        "        response = requests.get(link)\n",
        "        pdf = open(os.path.join(folder_path,filename), 'wb')\n",
        "        pdf.write(response.content)\n",
        "        pdf.close()\n"
      ],
      "metadata": {
        "id": "LDwZfSFw8Way"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9EIu_Q6isI-C"
      },
      "outputs": [],
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "\n",
        "def crawl_url(url, folder_path):\n",
        "  #to check type of url\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "    content_type = r.headers.get('content-type')\n",
        "#pdf file\n",
        "    if 'application/pdf' in content_type:\n",
        "      download_pdf(url, folder_path)\n",
        "#html file\n",
        "    #elif 'text/html' in content_type:\n",
        "    else:\n",
        "        filename = generate_file_name(url)\n",
        "        if exists(filename):\n",
        "          print('file already exists')\n",
        "          return\n",
        "        # Fetch the website content using requests or another library.\n",
        "        html_content = fetch_html_content(url)\n",
        "        soup=BeautifulSoup(html_content,\"html.parser\")\n",
        "\n",
        "        # Clean the HTML content using the clean_html function.\n",
        "        extracted_text = clean_html_and_extract_text(html_content)\n",
        "        # Store the HTML content and extracted text in separate files within the folder.\n",
        "        # here filename is the crawled_url_in_underscores\n",
        "\n",
        "        save_to_file(soup.prettify(), folder_path, f\"{filename}.html\")\n",
        "        save_to_file(extracted_text, folder_path, f\"{filename}.txt\")\n",
        "    #else:\n",
        "     # print('Unknown type: {}'.format(content_type))\n",
        "  except Exception as e:\n",
        "      print(url,e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVJZg209sVk2"
      },
      "outputs": [],
      "source": [
        "#main function getting sitemap_path\n",
        "import os\n",
        "import requests\n",
        "\n",
        "folder_path=\"/content/drive/MyDrive/crawled_data\"\n",
        "sitemap_path=\"https://raw.githubusercontent.com/Acuration/acuration-data-store/main/honeywell_sitemap.xml\"\n",
        "if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "\n",
        "urls_list=parse_sitemap(sitemap_path)\n",
        "\n",
        "for link in urls_list:\n",
        "    #cleaning and extracting data\n",
        "    crawl_url(link,\"/content/drive/MyDrive/crawled_data\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from os.path import exists\n",
        "\n",
        "link=\"https://honeywell.com/content/dam/honeywellbt/en/documents/downloads/Honeywell-Hungary-%C3%89ves-energetikai-szakreferensi-riport-2020-Magyar.pdf\"\n",
        "response = requests.get(link)\n",
        "pdf = open(os.path.join(\"/content/demo\",\"name\"), 'wb')\n",
        "pdf.write(response.content)\n",
        "pdf.close()"
      ],
      "metadata": {
        "id": "FyCSudxA30Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, os.path\n",
        "\n",
        "# simple version for working with CWD\n",
        "print (len([name for name in os.listdir('.') if os.path.isfile(name)]))\n",
        "\n",
        "# path joining version for other paths\n",
        "DIR = '/content/drive/MyDrive/crawled_data/pdfs'\n",
        "print (len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]))\n"
      ],
      "metadata": {
        "id": "W6XPgfVxMJ_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDlEyTPPwQv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQY6BDZEHha1"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vgaQ1ysZgCy_XkMXuYAHVmkysFCFhues",
      "authorship_tag": "ABX9TyPMCGijCRhDr7GG0Bi1ME3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}